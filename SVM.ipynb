{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the summary of the [cs229 lectures notes](http://cs229.stanford.edu/notes/cs229-notes3.pdf) by Andrew Ng.\n",
    "\n",
    "**References:**\n",
    "\n",
    "1. [cs229 lectures notes](http://cs229.stanford.edu/notes/cs229-notes3.pdf) by Andrew Ng.\n",
    "\n",
    "2. [MIT lecture notes](https://ocw.mit.edu/courses/mechanical-engineering/2-854-introduction-to-manufacturing-systems-fall-2016/lecture-notes/MIT2_854F16_KktExample.pdf)\n",
    "\n",
    "3. [Karush-Kuhn-Tucker (KKT) conditions](http://www.onmyphd.com/?p=kkt.karush.kuhn.tucker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "\n",
    "Logistic regression models $p(y = 1 / x; \\theta)$\n",
    "\n",
    "$$h_\\theta(x) = g(\\theta^{t}x)$$\n",
    "\n",
    "$$y = 1 \\ if \\ h_\\theta(x) \\ >= \\ 0.5 \\ and \\ 0 \\ otherwise$$\n",
    "\n",
    "This implies that,\n",
    "\n",
    "If $\\theta^{t}x$ is very large positive then we are very confident that it is a positive labelled example and if $\\theta^{t}x$ is very large negative then we are very confident that it is a negative labelled example.\n",
    "\n",
    "Hence, our objective is to find a seperating hyperplane ($\\theta^{t}x = 0$) which is far from all the positive and negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SVM, we use {-1, +1} instead of {0, 1} to denote the class labels.\n",
    "\n",
    "Similar to logistic regression,\n",
    "\n",
    "$$h_{w, b}(x) = g(w^{t}x + b)$$\n",
    "\n",
    "$$g(w^{t}x + b) = 1 \\ if \\ w^{t}x + b \\ >= \\ 0 \\ and \\ 0 \\ otherwise$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functional Margin:**\n",
    "\n",
    "$$\\hat{\\gamma}^{(i)} = y^{(i)}(w^{t}x+b)$$\n",
    "\n",
    "If $\\hat{\\gamma}^{(i)} > 0$ then our prediction for this trainig example is correct\n",
    "\n",
    "function margin is the minimum of all functional margins across training examples\n",
    "\n",
    "$$Min_{i} \\ \\hat{\\gamma}^{(i)}$$\n",
    "\n",
    "Main drawback of functional margin is, the model parameters w and b are not scale invariant. Meaning, if we multiply w and b with 2 then $h_{w, b}(x)$ will change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geometrical Margin:**\n",
    "\n",
    "$$\\gamma^{(i)} = y^{(i)}\\Big(\\big(\\frac{w}{||w||}\\big)^{t}x+\\frac{b}{||w||}\\Big)$$\n",
    "\n",
    "To overcome from the drawback of functional margin, we need to scale the model parameters\n",
    "\n",
    "Geometric margin is the minimum of all geometric margins across training examples\n",
    "\n",
    "$$Min_{i} \\ \\gamma^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal Margin Classifier:**\n",
    "\n",
    "**Problem Formulation:**\n",
    "\n",
    "$$Max_{\\gamma, w, b} \\gamma$$\n",
    "\n",
    "subject to,\n",
    "\n",
    "$$y^{(i)}(w^{t}x^{(i)}+b) \\ge \\gamma \\ \\forall  \\ i \\ \\in S$$ \n",
    "\n",
    "$$||w|| = 1$$\n",
    "\n",
    "Where, S is the training data with set of features and labels\n",
    "\n",
    "Second constraint ($||w|| = 1$) ensures that functional margin equals to geometrical margin. Hence, the model parameters are scale invariant\n",
    "\n",
    "Here we are trying to find optimal w and b for which $\\gamma$ is maximum and $y^{(i)}(w^{t}x^{(i)}+b) \\ge \\gamma$ for all the training examples\n",
    "\n",
    "It is very difficult to solve the above problem because $||w|| = 1$ is non-convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplified version of the above model:\n",
    "\n",
    "$$Max_{\\gamma, w, b} \\frac{\\gamma}{||w||}$$\n",
    "\n",
    "subject to,\n",
    "\n",
    "$$y^{(i)}(w^{t}x^{(i)}+b) \\ge \\gamma \\ \\forall  \\ i \\ \\in S$$ \n",
    "\n",
    "$\\frac{1}{||W||}$ in the objective function ensures that functional margin equals to geometrical margin\n",
    "\n",
    "We removed the constraint $||w|| = 1$ and simplified the problem but still it is non-convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed earlier, model parameters w and b of the above problem are scale invariant and $\\gamma$ is an arbitary model parameter. Hence, we can set $\\gamma = 1$. Objective $Max_{w, b} \\frac{1}{||w||}$ is non-convex so changing the objective to $Min_{w, b} \\frac{||w||^2}{2}$.\n",
    "\n",
    "Simplified version of the above model:\n",
    "\n",
    "$$Min_{w, b} \\frac{||w||^2}{2}$$\n",
    "\n",
    "subject to,\n",
    "\n",
    "$$y^{(i)}(w^{t}x^{(i)}+b) \\ge 1 \\ \\forall  \\ i \\ \\in S$$ \n",
    "\n",
    "The above problem can be solved using Quadratic Programming solvers. In the next section we will discuss about Lagrangian Duality, which will help us in solving the above optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lagrangian Duality:**\n",
    "\n",
    "Lagrangian is the process of converting a constrained optimization problem into unconstrained optimization problem using lagrangian multipliers\n",
    "\n",
    "Let us consider the below optimization problem:\n",
    "\n",
    "$$Min_{w} f(w)$$\n",
    "\n",
    "subject to,\n",
    "\n",
    "$$g_{i}(w) \\le 0 \\ \\forall \\ i$$\n",
    "\n",
    "$$h_{i}(w) = 0 \\ \\forall \\ i$$\n",
    "\n",
    "Lagrangian of the above problem is,\n",
    "\n",
    "$$L(\\alpha, \\beta, w) = f(w) + \\sum_{i}\\alpha_{i}g_{i}(w) + \\sum_{i}\\beta_{i}h_{i}(w)$$\n",
    "\n",
    "Where $\\alpha_i$s and $\\beta_i$s are lagrangian multipliers\n",
    "\n",
    "**Note:** If your familiar with soft constraints in the optimization problem, lagrangian is very similar to that but here we assign dynamic cost (model parameter) for each constraint, whereas in the soft constraint we assign one user defined cost (hyper parameter) for group of soft constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primal Problem:**\n",
    "\n",
    "$$\\theta_P\\big(w\\big) = Max_{ \\ \\alpha \\ge 0 \\ ; \\ \\beta} \\ L(\\alpha, \\beta, w)$$\n",
    "\n",
    "If any of the constraint fails for any w then $\\theta_p\\big(w\\big)$ returns $\\infty$, otherwise it returns $f(w)$\n",
    "\n",
    "If any of the $g_{i}(w) \\le 0$ constraint $\\implies$ there exist at least one w and i for which $g_{i}(w) > 0$\n",
    "\n",
    "Hence, our original problem can be re-written as follows:\n",
    "\n",
    "$$Min_{w} \\theta_{p}\\big(w\\big) = Min_{w} \\ Max_{ \\ \\alpha \\ge 0 \\ ; \\ \\beta} \\ L(\\alpha, \\beta, w)$$\n",
    "\n",
    "**Note:** Primal problem first focuses on getting feasible region as a function of $w$ by maximizing $L(\\alpha, \\beta, w)$ with respect to $\\alpha, \\beta$ and then finding optimal solution within the feasible region by minimizing $\\theta_P\\big(w\\big)$ with respect to $w$. However, this is my understanding of the primal problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Dual Problem:**\n",
    "\n",
    "$$\\theta_D\\big(w\\big) = Min_{\\ w} \\ L(\\alpha, \\beta, w)$$\n",
    "\n",
    "Dual optimization problem:\n",
    "\n",
    "$$Max_{ \\ \\alpha \\ge 0 \\ ; \\ \\beta} \\ \\theta_D\\big(w\\big) = Max_{ \\ \\alpha \\ge 0 \\ ; \\ \\beta} \\ Min_{\\ w} \\ L(\\alpha, \\beta, w)$$\n",
    "\n",
    "**Note:** Dual problem first focuses on getting optimal solution as a function of $\\alpha, \\ \\beta$ by minimizing $L(\\alpha, \\beta, w)$ with respect to $w$ and then finding fesible solution by maximizing $\\theta_D\\big(w\\big)$ with respect to $\\alpha,  \\ \\beta$. However, this is my understanding of the dual problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KKT Conditions:**\n",
    "\n",
    "Let $p^{*}$ be the primal solution and $d^{*}$ be the dual solution then $d^{*} \\le p^{*}$. Equality holds if the solution of primal and dual satisfies Karush-Kuhn-Tucker (KKT) conditions.\n",
    "\n",
    "Let $w^{*}$ be the solution to primal problem and $\\alpha^{*}, \\beta^{*}$ are the solutions to the dual problem then $p^{*} = d^{*}$ if $w^{*}, \\alpha^{*}, \\beta^{*}$ satisfies below KKT conditions:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{i}} L \\ (w^{*}, \\alpha^{*}, \\beta^{*}) = 0 \\ \\forall \\ i = 1, 2, ... n$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_{i}} L \\ (w^{*}, \\alpha^{*}, \\beta^{*}) = 0 \\ \\forall \\ i = 1, 2, ... k$$\n",
    "\n",
    "$$g_{i}(w^{*}) \\le 0 \\ \\forall \\ i = 1, 2, ... m$$\n",
    "\n",
    "$$\\alpha_i^{*}g_i(w^*) = 0 \\ \\forall \\ i = 1, 2, ... m \\ \\ ^*$$\n",
    "\n",
    "$$\\alpha_i^{*} \\ge 0 \\ \\forall \\ i = 1, 2, ... m$$\n",
    "\n",
    "\\* - KKT dual complementarity condition: If $g_i(w^*) < 0$ then it will not impact optimization even we remove the constraint so we set $\\alpha_i = 0 \\ \\forall \\ g_i(w^*) < 0$. If $g_i(w^*) = 0$ then $\\alpha_i^* \\ge 0$. Hence, in both the cases either of $g_i(w^*)$ or $\\alpha_i^*$ is zero.\n",
    "\n",
    "To understand more about KKT conditions please refer references [2] and [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal Margin Classifier (Cont...) :**\n",
    "\n",
    "Recall the optimal margin classifier discussed above\n",
    "\n",
    "$$Max_{w} \\frac{||w||^2}{2}$$\n",
    "\n",
    "subject to,\n",
    "\n",
    "$$y^{(i)}(w^{t}x^{(i)}+b) \\ge 1 \\ \\forall  \\ i \\ \\in S$$ \n",
    "\n",
    "Lagrangian of this optimization problem can be written as follows:\n",
    "\n",
    "$$L(w, b, \\alpha) = \\frac{||w||^2}{2} - \\sum_{i} \\alpha_{i} \\big[y^{(i)}(w^{t}x^{(i)}+b) - 1\\big]$$\n",
    "\n",
    "Where $\\alpha_i$s are lagrangian multipliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:**\n",
    "\n",
    "Differentiate $L(w, b, \\alpha)$ w.r.t $w$ and equate it to 0\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w}} = w - \\sum_{i} \\alpha_i y_i x_i$$\n",
    "\n",
    "$$w = \\sum_{i} \\alpha_i y_i x_i$$\n",
    "\n",
    "**Step 2:**\n",
    "\n",
    "Differentiate $L(w, b, \\alpha)$ w.r.t $b$ and equate it to 0\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = \\sum_{i} \\alpha_i y_i$$\n",
    "\n",
    "$$\\sum_{i} \\alpha_i y_i = 0$$\n",
    "\n",
    "**Step 3:**\n",
    "\n",
    "Substitute $w$ in $L(w, b, \\alpha)$ then we get\n",
    "\n",
    "$$L(w, b, \\alpha) = \\sum_i \\alpha_i - \\frac{1}{2} \\sum_{i, j} y_i y_j \\alpha_i \\alpha_j x_i^{t} x_j - b \\sum_i \\alpha_i y_i$$\n",
    "\n",
    "we know that, $\\sum_{i} \\alpha_i y_i = 0$ and hence\n",
    "\n",
    "$$L(w, b, \\alpha) = \\sum_i \\alpha_i - \\frac{1}{2} \\sum_{i, j} y_i y_j \\alpha_i \\alpha_j x_i^{t} x_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dual Optimization Problem:**\n",
    "\n",
    "$$Max_{\\alpha} W(\\alpha) =  \\sum_i \\alpha_i - \\frac{1}{2} \\sum_{i, j} y_i y_j \\alpha_i \\alpha_j x_i^{t} x_j$$\n",
    "\n",
    "Subject to,\n",
    "\n",
    "$$\\alpha_i \\ge 0 \\ \\forall \\ i$$\n",
    "\n",
    "$$\\sum_i \\alpha_i y_i = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get the optimal $\\alpha_i$s from the above optimization problem then we can calculate $w^t x + b$ as follows: \n",
    "\n",
    "$$w^t x + b = \\Big(\\sum_i \\alpha_i y_i x_i\\Big)^t x + b$$\n",
    "\n",
    "$$w^t x + b = \\sum_i \\alpha_i y_i x_i^t x + b$$\n",
    "\n",
    "Where $\\alpha_i = 0$ for all points other than support vectors (from dual complementarity condition). Hence, to classify a new data point we just have to take the inner product between new data point and support vectors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
